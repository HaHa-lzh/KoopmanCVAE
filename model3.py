import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)

# ==========================================
# æ ¸å¿ƒæ¨¡å‹ç»“æ„ï¼ˆCVAE+Koopmanå¾®åˆ†ç®—å­+æ ‡ç­¾åŒ–ç›‘ç£ï¼‰
# ==========================================
class KoopmanCVAE(nn.Module):  # é‡å‘½åä¸ºKoopmanCVAEï¼Œä½“ç°CVAEæ ¸å¿ƒ
    def __init__(self, state_dim, obs_dim, k_latent_dim, hidden_dim=128):
        """
        é‡æ„åæ ¸å¿ƒé€»è¾‘ï¼š
        1. ç¼–ç å™¨ï¼šXâ†’Psiï¼ˆæ— ä¿®æ”¹ï¼‰
        2. ç®—å­æ¨ç†ï¼ˆCVAEç¼–ç å™¨ï¼‰ï¼š(psi_i, psi_{i+1}, Î”t_i) â†’ å¾®åˆ†ç®—å­åˆ†å¸ƒ(mu, log_var)
        3. CVAEæ ‡ç­¾ï¼š(X_i, Î”t_i) ä½œä¸ºç”Ÿæˆç½‘ç»œçš„æ¡ä»¶æ ‡ç­¾
        4. ç®—å­ç”Ÿæˆï¼ˆCVAEè§£ç å™¨ï¼‰ï¼š(z, æ ‡ç­¾) â†’ Koopmanç®—å­çŸ©é˜µK
        5. é‡æ„çº¦æŸï¼šK * psi_i â‰ˆ psi_{i+1}
        Args:
            state_dim (int): åŸå§‹çŠ¶æ€Xç»´åº¦ï¼ˆè½¨é“6ç»´ï¼‰
            obs_dim (int): Koopmanè§‚æµ‹ç©ºé—´Psiç»´åº¦
            k_latent_dim (int): Koopmanå¾®åˆ†ç®—å­æ½œåœ¨ç»´åº¦
            hidden_dim (int): éšè—å±‚ç¥ç»å…ƒæ•°é‡
        """
        super(KoopmanCVAE, self).__init__()
        
        self.state_dim = state_dim
        self.obs_dim = obs_dim
        self.k_latent_dim = k_latent_dim
        
        # 1. ç¼–ç å™¨ F: X â†’ Psiï¼ˆå®Œå…¨ä¿ç•™åŸæœ‰ç»“æ„ï¼‰
        self.encoder_f = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(), 
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, obs_dim)
        )
        
        # 2. CVAEç®—å­æ¨ç†ç½‘ç»œï¼ˆç¼–ç å™¨ï¼‰ï¼š(psi_i, psi_{i+1}, Î”t) â†’ (mu, log_var)
        # è¾“å…¥ï¼špsi_i + psi_{i+1} + Î”tï¼ˆè½¬ç§»å¯¹ä¸‰è¦ç´ ï¼‰
        self.cvae_encoder = nn.Sequential(
            nn.Linear(obs_dim * 2 + 1, hidden_dim * 2),  # obs_dim*2=psi_i+psi_{i+1}, +1=Î”t
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim * 2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, k_latent_dim * 2)  # è¾“å‡ºå¾®åˆ†ç®—å­åˆ†å¸ƒå‚æ•°
        )
        
        # 3. CVAEç®—å­ç”Ÿæˆç½‘ç»œï¼ˆè§£ç å™¨ï¼‰ï¼šz + æ ‡ç­¾(X_i, Î”t) â†’ KoopmançŸ©é˜µ
        # æ ‡ç­¾ç»´åº¦ï¼šstate_dim(X_i) + 1(Î”t)
        self.cvae_decoder = nn.Sequential(
            nn.Linear(k_latent_dim + state_dim + 1, hidden_dim * 2),  # z + æ ‡ç­¾
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim * 2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, obs_dim * obs_dim)  # è¾“å‡ºKoopmançŸ©é˜µå±•å¹³
        )
        
        # 4. è§£ç å™¨ F': Psi â†’ Xï¼ˆå®Œå…¨ä¿ç•™åŸæœ‰ç»“æ„ï¼‰
        self.decoder_f_prime = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, state_dim)
        )

    def reparameterize(self, mu, log_var):
        """é‡å‚æ•°åŒ–æŠ€å·§ï¼ˆæ— ä¿®æ”¹ï¼‰"""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def get_koopman_matrix(self, z, label):
        """
        CVAEç”ŸæˆKoopmanç®—å­çŸ©é˜µ
        Args:
            z: (batch_size, k_latent_dim) é‡é‡‡æ ·çš„æ½œåœ¨ç‰¹å¾
            label: (batch_size, state_dim + 1) æ ‡ç­¾(X_i, Î”t)
        Returns:
            K_matrix: (batch_size, obs_dim, obs_dim) Koopmanç®—å­çŸ©é˜µ
        """
        batch_size = z.size(0)
        z_with_label = torch.cat([z, label], dim=1)  # æ‹¼æ¥æ½œåœ¨ç‰¹å¾å’Œæ ‡ç­¾
        k_flat = self.cvae_decoder(z_with_label)
        K_matrix = k_flat.view(batch_size, self.obs_dim, self.obs_dim)
        return K_matrix
    
    # ===================== æ–°å¢æ–¹æ³•ï¼šè®¡ç®—KçŸ©é˜µé€æ¬¡ä¹˜ç§¯ï¼ˆé€‚é…éç›¸é‚»çŠ¶æ€å¯¹ï¼‰ =====================
    def compute_k_product(self, k_matrices_list):
        """
        æ‰¹é‡è®¡ç®—å¤šæ­¥KçŸ©é˜µçš„é€æ¬¡ä¹˜ç§¯ï¼ˆç”¨äºéç›¸é‚»çŠ¶æ€å¯¹çš„å¤šæ­¥è¯¯å·®è®¡ç®—ï¼‰
        Args:
            k_matrices_list: list of tensorï¼Œæ¯ä¸ªtensorå½¢çŠ¶ä¸º(batch_size, obs_dim, obs_dim)
                            å¯¹åº”(Xi,Xi+1),(Xi+1,Xi+2)...(Xj-1,Xj)çš„KçŸ©é˜µ
        Returns:
            K_product: (batch_size, obs_dim, obs_dim) é€æ¬¡ä¹˜ç§¯ç»“æœ K_i * K_{i+1} * ... * K_{j-1}
        """
        if not k_matrices_list:  # ç©ºåˆ—è¡¨å…œåº•
            return torch.eye(self.obs_dim).unsqueeze(0).repeat(
                1, 1, 1).to(next(self.parameters()).device)
        
        # åˆå§‹åŒ–ä¹˜ç§¯ä¸ºå•ä½çŸ©é˜µï¼ˆåŒ¹é…batch_sizeï¼‰
        batch_size = k_matrices_list[0].shape[0]
        K_product = torch.eye(self.obs_dim).unsqueeze(0).repeat(
            batch_size, 1, 1).to(k_matrices_list[0].device)
        
        # é€æ¬¡çŸ©é˜µä¹˜æ³•ï¼ˆæŒ‰é¡ºåºç›¸ä¹˜ï¼‰
        for K in k_matrices_list:
            K_product = torch.bmm(K_product, K)
        
        return K_product

    def forward_single_pair(self, x_i, x_i_plus_1, delta_t):
        """
        å•è½¬ç§»å¯¹å‰å‘ï¼ˆæ ¸å¿ƒé‡æ„ï¼‰ï¼š(X_i, X_{i+1}, Î”t) â†’ ç®—å­+é‡æ„
        Args:
            x_i: (batch_size, state_dim) å‰ä¸€æ—¶åˆ»çŠ¶æ€
            x_i_plus_1: (batch_size, state_dim) åä¸€æ—¶åˆ»çŠ¶æ€
            delta_t: (batch_size, 1) æ—¶é—´é—´éš”
        Returns:
            psi_i: X_içš„ç¼–ç 
            psi_i_plus_1: X_{i+1}çš„ç¼–ç 
            K_matrix: å¯¹åº”æ ‡ç­¾çš„Koopmanç®—å­
            psi_recon: K*psi_iï¼ˆé‡æ„åpsi_{i+1}ï¼‰
            x_i_recon: psi_iè§£ç å›X_i
            x_i_plus_1_recon: psi_reconè§£ç å›X_{i+1}
            mu/log_var: å¾®åˆ†ç®—å­åˆ†å¸ƒå‚æ•°
        """
        # ========== æ–°å¢ï¼šå¼ºåˆ¶ç»Ÿä¸€æ‰€æœ‰è¾“å…¥ç»´åº¦ ==========
        # ç¡®ä¿çŠ¶æ€é‡æ˜¯2D (B, 6)
        if x_i.dim() == 1:
            x_i = x_i.unsqueeze(0)  # å•æ ·æœ¬ï¼š(6,) â†’ (1,6)
        if x_i_plus_1.dim() == 1:
            x_i_plus_1 = x_i_plus_1.unsqueeze(0)  # (6,) â†’ (1,6)
        
        # ç¡®ä¿delta_tæ˜¯2D (B, 1) â€”â€” æ ¸å¿ƒä¿®å¤
        if delta_t.dim() == 1:
            delta_t = delta_t.unsqueeze(1)  # (B,) â†’ (B,1) æˆ– (1,) â†’ (1,1)
        
        # ========== åŸæœ‰é€»è¾‘ ==========
        # ç¼–ç çŠ¶æ€é‡
        psi_i = self.encoder_f(x_i)          # 2D (B, obs_dim)
        psi_i_plus_1 = self.encoder_f(x_i_plus_1)  # 2D (B, obs_dim)
        
        # ========== æ­¤æ—¶æ‰€æœ‰tensoréƒ½æ˜¯2Dï¼Œæ‹¼æ¥æˆåŠŸ ==========
        cvae_input = torch.cat([psi_i, psi_i_plus_1, delta_t], dim=1)

        # 2. CVAEç¼–ç å™¨ï¼šæ¨ç†å¾®åˆ†ç®—å­åˆ†å¸ƒ
        k_params = self.cvae_encoder(cvae_input)
        mu, log_var = torch.chunk(k_params, 2, dim=1)
        
        # 3. é‡é‡‡æ ·æ½œåœ¨ç‰¹å¾z
        z = self.reparameterize(mu, log_var)
        
        # 4. æ„å»ºCVAEæ ‡ç­¾ï¼š(X_i, Î”t)
        label = torch.cat([x_i, delta_t], dim=1)
        
        # 5. CVAEè§£ç å™¨ï¼šç”ŸæˆKoopmanç®—å­
        K_matrix = self.get_koopman_matrix(z, label)
        
        # 6. å•æ­¥é‡æ„ï¼šK*psi_i â‰ˆ psi_{i+1}
        psi_recon = torch.bmm(K_matrix, psi_i.unsqueeze(2)).squeeze(2)
        
        # 7. è§£ç å›Xç©ºé—´
        x_i_recon = self.decoder_f_prime(psi_i)
        x_i_plus_1_recon = self.decoder_f_prime(psi_recon)
        
        return psi_i, psi_i_plus_1, K_matrix, psi_recon, x_i_recon, x_i_plus_1_recon, mu, log_var

    def predict_long_term(self, x0, delta_t_seq, steps, use_prior=True, z_mean=None):
        """
        ä¿®æ­£åé•¿æ—¶é¢„æµ‹ï¼šåŸºäºåˆå§‹X0å’ŒÎ”tåºåˆ—ï¼Œä»…ç”¨CVAEè§£ç å™¨ï¼ˆæ ‡ç­¾â†’ç®—å­ï¼‰åšé¢„æµ‹
        Args:
            x0: (1, state_dim) åˆå§‹çŠ¶æ€ï¼ˆbatch_size=1ï¼‰
            delta_t_seq: (N, 1) æ—¶é—´é—´éš”åºåˆ—ï¼ˆnumpy/tensorï¼‰
            steps: int é¢„æµ‹æ­¥æ•°
            use_prior: bool æ˜¯å¦ä½¿ç”¨å…ˆéªŒåˆ†å¸ƒï¼ˆN(0,1)ï¼‰é‡‡æ ·zï¼ŒFalseåˆ™ç”¨è¾“å…¥çš„z_mean
            z_mean: (1, k_latent_dim) è‡ªå®šä¹‰æ½œåœ¨ç‰¹å¾ï¼ˆå¦‚è®­ç»ƒæ—¶çš„å‡å€¼ï¼‰
        Returns:
            trajectory: (steps+1, state_dim) é¢„æµ‹è½¨è¿¹
            K_matrices: list æ¯æ­¥çš„Koopmanç®—å­
        """
        self.eval()
        # æ ¸å¿ƒï¼šæå‰è·å–åŸºå‡†è®¾å¤‡ï¼ˆä»x0æå–ï¼Œç¡®ä¿æ‰€æœ‰å¼ é‡ç»Ÿä¸€ï¼‰
        device = x0.device
        trajectory = [x0[0].cpu().numpy()]  # åˆå§‹çŠ¶æ€
        K_matrices = []

        print("\n" + "="*50)
        print("ğŸ” delta_t_seq åºåˆ—è¯¦ç»†ä¿¡æ¯")
        print("="*50)
        
        # 1. åŸºç¡€ä¿¡æ¯ï¼ˆç±»å‹ã€è®¾å¤‡ã€ç»´åº¦ï¼‰
        if isinstance(delta_t_seq, np.ndarray):
            print(f"åŸå§‹ç±»å‹ï¼šnumpy.ndarray | åŸå§‹å½¢çŠ¶ï¼š{delta_t_seq.shape}")
            # è½¬æ¢ä¸ºtensorç”¨äºç»Ÿä¸€å¤„ç†
            delta_t_seq_tensor = torch.FloatTensor(delta_t_seq).to(device)
        elif isinstance(delta_t_seq, torch.Tensor):
            print(f"åŸå§‹ç±»å‹ï¼štorch.Tensor | åŸå§‹è®¾å¤‡ï¼š{delta_t_seq.device} | åŸå§‹å½¢çŠ¶ï¼š{delta_t_seq.shape}")
            delta_t_seq_tensor = delta_t_seq.to(device)
        else:
            print(f"åŸå§‹ç±»å‹ï¼š{type(delta_t_seq)}ï¼ˆä¸æ”¯æŒï¼‰")
            delta_t_seq_tensor = torch.tensor([], device=device)
        
        # 2. ç»Ÿä¸€æ ¼å¼åçš„ä¿¡æ¯
        if len(delta_t_seq_tensor) > 0:
            # å±•å¹³ä¸º1Dä¾¿äºæ‰“å°å’Œç»Ÿè®¡
            delta_t_seq_flat = delta_t_seq_tensor.cpu().numpy().flatten()
            print(f"ç»Ÿä¸€åå½¢çŠ¶ï¼š{delta_t_seq_tensor.shape} | å±•å¹³åé•¿åº¦ï¼š{len(delta_t_seq_flat)}")
            print(f"é¢„æµ‹æ­¥æ•°ï¼š{steps} | Î”tåºåˆ—é•¿åº¦æ˜¯å¦åŒ¹é…ï¼š{'âœ…' if len(delta_t_seq_flat) >= steps else 'âŒ'}")
            
            # 3. æ‰“å°å‰100ä¸ªå€¼ï¼ˆä¸è¶³100åˆ™æ‰“å°å…¨éƒ¨ï¼‰
            print("\nğŸ“Š å‰100ä¸ªÎ”tå€¼ï¼ˆæ¯è¡Œ10ä¸ªï¼‰ï¼š")
            print_limit = min(100, len(delta_t_seq_flat))
            delta_t_print = delta_t_seq_flat[:print_limit]
            for i in range(0, print_limit, 10):
                end_idx = min(i+10, print_limit)
                dt_line = [f"{dt:.6f}" for dt in delta_t_print[i:end_idx]]
                print(f"  ç¬¬{i+1}-{end_idx}ä¸ªï¼š{', '.join(dt_line)}")
        else:
            print("âŒ Î”tåºåˆ—ä¸ºç©ºï¼")
        print("="*50 + "\n")

        # ç»Ÿä¸€æ•°æ®ç±»å‹å’Œè®¾å¤‡ï¼šç¡®ä¿delta_t_seqå…¨ç¨‹åœ¨æŒ‡å®šè®¾å¤‡ä¸Š
        if isinstance(delta_t_seq, np.ndarray):
            delta_t_seq = torch.FloatTensor(delta_t_seq).to(device)  # æ”¹ä¸ºç”¨ç»Ÿä¸€çš„device
        else:
            delta_t_seq = delta_t_seq.to(device)  # è¡¥å……ï¼šå¦‚æœè¾“å…¥æ˜¯tensorï¼Œä¹Ÿå¼ºåˆ¶è½¬åˆ°ç›®æ ‡è®¾å¤‡

        x_curr = x0.to(device)  # æ˜¾å¼ç¡®ä¿x_curråœ¨ç›®æ ‡è®¾å¤‡
        
        with torch.no_grad():
            for i in range(steps):
                # 1. è·å–å½“å‰æ­¥çš„Î”tï¼ˆå…œåº•å¤„ç†ï¼šå…³é”®ä¿®å¤â€”â€”æŒ‡å®šè®¾å¤‡ï¼‰
                take_len = min(i, len(delta_t_seq))  # é¿å…ç´¢å¼•è¶Šç•Œ
                delta_t_prev = delta_t_seq[:take_len]  # å–å‰take_lenä¸ªå€¼
                delta_t_sum = delta_t_prev.sum().item()  # è®¡ç®—ç´¯åŠ å’Œ
                
                # ç”Ÿæˆç´¯åŠ å’Œçš„Î”t tensorï¼ˆæŒ‡å®šdeviceï¼‰
                delta_t = torch.tensor([[delta_t_sum]], dtype=torch.float32, device=device)
                
                # ===================== æ‰“å°å…œåº•delta_t =====================
                #print(f"ç¬¬{i+1}æ­¥ | delta_t = {delta_t_sum:.6f} ""(å‰{take_len}ä¸ªå€¼çš„ç´¯åŠ å’Œ)")

                # 2. æ„å»ºCVAEæ ‡ç­¾ï¼šæ­¤æ—¶x_currå’Œdelta_tå·²åœ¨åŒä¸€è®¾å¤‡
                label = torch.cat([x_curr, delta_t], dim=1)  # (1, state_dim + 1)
                
                # 3. æ½œåœ¨ç©ºé—´é‡‡æ ·ï¼ˆç»Ÿä¸€è®¾å¤‡ï¼‰
                if use_prior:
                    z = torch.randn(1, self.k_latent_dim, dtype=torch.float32, device=device)
                else:
                    if z_mean is None:
                        z = torch.zeros(1, self.k_latent_dim, dtype=torch.float32, device=device)
                    else:
                        z = z_mean.to(device)  # ç¡®ä¿z_meanä¹Ÿè½¬åˆ°ç›®æ ‡è®¾å¤‡
                
                # 4. ä»…ç”¨CVAEè§£ç å™¨ç”ŸæˆKoopmanç®—å­
                K_matrix = self.get_koopman_matrix(z, label)  # (1, obs_dim, obs_dim)
                K_matrices.append(K_matrix.cpu().numpy())
                
                # 5. Koopmanç®—å­æ˜ å°„ï¼špsi_curr â†’ psi_next
                psi_curr = self.encoder_f(x_curr)  # (1, obs_dim)
                psi_next = torch.bmm(K_matrix, psi_curr.unsqueeze(2)).squeeze(2)  # (1, obs_dim)
                
                # 6. è§£ç å›Xç©ºé—´
                x_next = self.decoder_f_prime(psi_next)  # (1, state_dim)
                
                # 7. æ›´æ–°è½¨è¿¹å’Œå½“å‰çŠ¶æ€
                trajectory.append(x_next[0].cpu().numpy())
                #x_curr = x_next  # è¿­ä»£æ›´æ–°å½“å‰çŠ¶æ€ï¼ˆå·²åœ¨deviceä¸Šï¼‰
        
        return np.array(trajectory), K_matrices
    

#  ==========================================
# æŸå¤±å‡½æ•°ï¼ˆé‡æ„ï¼šæŒ‰æ–‡ä»¶åˆ†ç»„çš„mu/log_varä¸€è‡´æ€§çº¦æŸï¼‰
# ==========================================
def loss_cvae_koopman(psi_recon, psi_true, 
                      x_i_recon, x_i_true,
                      x_i_plus_1_recon, x_i_plus_1_true,
                      mu, log_var, K_matrix,
                      # ===================== æ–°å¢ï¼šæŒ‰æ–‡ä»¶åˆ†ç»„çš„æ ¸å¿ƒå‚æ•° =====================
                      file_ids,  # æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ–‡ä»¶ç¼–å· (batch_size,)
                      # ===================== åŸæœ‰å‚æ•° =====================
                      K_product=None, lambda_k_product=10.0,
                      alpha=1.0, beta=0.1, gamma=1, theta=0.1, delta=1.0):
    """
    æ‰©å±•åæŸå¤±å‡½æ•°ï¼š
    1. psi_recon_loss: K*psi_i â‰ˆ psi_{i+1}ï¼ˆæ ¸å¿ƒå•æ­¥é‡æ„ï¼‰
    2. x_recon_loss: Xé‡æ„æŸå¤±
    3. kl_loss: CVAEæ½œåœ¨åˆ†å¸ƒKLæ•£åº¦
    4. k_product_loss: å¤šæ­¥Kä¹˜ç§¯è¯¯å·®ï¼ˆä»…K_productä¸ä¸ºNoneæ—¶ç”Ÿæ•ˆï¼‰
    5. latent_consistency_loss: æŒ‰æ–‡ä»¶åˆ†ç»„çº¦æŸmu/log_varï¼ˆåŒæ–‡ä»¶å†…æ ·æœ¬æ¥è¿‘ï¼‰
    Args:
        file_ids: (batch_size,) tensor - æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ–‡ä»¶ç¼–å·ï¼ˆå¦‚0,0,1,1,2...ï¼‰
        K_product: (batch_size, obs_dim, obs_dim) å¤šæ­¥KçŸ©é˜µä¹˜ç§¯
        lambda_k_product: å¤šæ­¥Kä¹˜ç§¯è¯¯å·®çš„æƒé‡
        delta: æŒ‰æ–‡ä»¶åˆ†ç»„çš„ä¸€è‡´æ€§æŸå¤±æƒé‡
    """
    # 1. æ ¸å¿ƒé‡æ„æŸå¤±ï¼šK*psi_i â‰ˆ psi_{i+1}
    psi_recon_loss = F.mse_loss(psi_recon, psi_true)
    
    # 2. Xç©ºé—´é‡æ„æŸå¤±
    x_i_recon_loss = F.mse_loss(x_i_recon, x_i_true)
    x_i_plus_1_recon_loss = F.mse_loss(x_i_plus_1_recon, x_i_plus_1_true)
    x_recon_loss = (x_i_recon_loss + x_i_plus_1_recon_loss) / 2
    
    # 3. CVAE KLæ•£åº¦æŸå¤±
    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    kl_loss = kl_loss / psi_recon.size(0)
    
    # ===================== åŸæœ‰ï¼šå¤šæ­¥Kä¹˜ç§¯è¯¯å·® =====================
    k_product_loss = torch.tensor(0.0).to(psi_recon.device)
    if K_product is not None:
        k_product_loss = F.mse_loss(K_matrix, K_product) * lambda_k_product
    
    # ===================== æ ¸å¿ƒä¿®æ”¹ï¼šæŒ‰æ–‡ä»¶åˆ†ç»„çš„mu/log_varä¸€è‡´æ€§æŸå¤± =====================
    latent_consistency_loss = torch.tensor(0.0).to(psi_recon.device)
    
    # è·å–å½“å‰batchä¸­çš„å”¯ä¸€æ–‡ä»¶ID
    unique_file_ids = torch.unique(file_ids)
    
    for fid in unique_file_ids:
        # ç­›é€‰å½“å‰æ–‡ä»¶IDå¯¹åº”çš„æ ·æœ¬ç´¢å¼•
        mask = (file_ids == fid)  # (batch_size,) bool tensor
        if not torch.any(mask):
            continue
        
        # æå–å½“å‰æ–‡ä»¶å†…çš„muå’Œlog_var
        mu_group = mu[mask]  # (n_group, latent_dim)
        log_var_group = log_var[mask]  # (n_group, latent_dim)
        
        # è®¡ç®—å½“å‰æ–‡ä»¶å†…çš„å…¨å±€å‡å€¼ï¼ˆä»…æœ¬ç»„æ ·æœ¬ï¼‰
        mu_group_mean = mu_group.mean(dim=0, keepdim=True)  # (1, latent_dim)
        log_var_group_mean = log_var_group.mean(dim=0, keepdim=True)  # (1, latent_dim)
        
        # çº¦æŸå½“å‰æ–‡ä»¶å†…çš„æ ·æœ¬å‘æœ¬ç»„å‡å€¼é æ‹¢
        mu_consistency = F.mse_loss(mu_group, mu_group_mean.expand_as(mu_group))
        log_var_consistency = F.mse_loss(log_var_group, log_var_group_mean.expand_as(log_var_group))
        
        # ç´¯åŠ å½“å‰æ–‡ä»¶çš„ä¸€è‡´æ€§æŸå¤±ï¼ˆæŒ‰æ ·æœ¬æ•°åŠ æƒï¼Œé¿å…å°æ–‡ä»¶å æ¯”è¿‡é«˜ï¼‰
        n_group = torch.sum(mask).float()
        latent_consistency_loss += (mu_consistency + log_var_consistency) * n_group
    
    # å½’ä¸€åŒ–ï¼ˆé™¤ä»¥æ€»æ ·æœ¬æ•°ï¼‰+ æƒé‡
    latent_consistency_loss = (latent_consistency_loss / mu.size(0)) * delta

    # æ€»æŸå¤±ï¼šåŸæœ‰æŸå¤± + æŒ‰æ–‡ä»¶åˆ†ç»„çš„ä¸€è‡´æ€§æŸå¤±
    total_loss = (psi_recon_loss * alpha + 
                  x_recon_loss * beta + 
                  kl_loss * gamma +
                  k_product_loss +
                  latent_consistency_loss)
    
    # è¿”å›æ‰©å±•çš„æŸå¤±é¡¹ï¼ˆå«åˆ†ç»„ä¸€è‡´æ€§æŸå¤±ï¼‰
    return total_loss, psi_recon_loss, x_recon_loss, kl_loss, k_product_loss, latent_consistency_loss