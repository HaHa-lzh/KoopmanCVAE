import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
torch.manual_seed(42)
np.random.seed(42)

# ==========================================
# æ ¸å¿ƒæ¨¡å‹ç»“æ„ï¼ˆCVAE+Koopmanå¾®åˆ†ç®—å­+æ ‡ç­¾åŒ–ç›‘ç£ï¼‰
# æ ¸å¿ƒä¿®æ”¹ï¼šCVAEæ ‡ç­¾ä»(Xi, Î”t)æ”¹ä¸º(file_id, Î”t)
# ==========================================
class KoopmanCVAE(nn.Module):  # é‡å‘½åä¸ºKoopmanCVAEï¼Œä½“ç°CVAEæ ¸å¿ƒ
    def __init__(self, state_dim, obs_dim, k_latent_dim, hidden_dim=128, max_file_id=100):
        """
        é‡æ„åæ ¸å¿ƒé€»è¾‘ï¼š
        1. ç¼–ç å™¨ï¼šXâ†’Psiï¼ˆæ— ä¿®æ”¹ï¼‰
        2. ç®—å­æ¨ç†ï¼ˆCVAEç¼–ç å™¨ï¼‰ï¼š(psi_i, psi_{i+1}, Î”t_i) â†’ å¾®åˆ†ç®—å­åˆ†å¸ƒ(mu, log_var)
        3. CVAEæ ‡ç­¾ï¼š(file_id, Î”t_i) ä½œä¸ºç”Ÿæˆç½‘ç»œçš„æ¡ä»¶æ ‡ç­¾ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
        4. ç®—å­ç”Ÿæˆï¼ˆCVAEè§£ç å™¨ï¼‰ï¼š(z, æ ‡ç­¾) â†’ Koopmanç®—å­çŸ©é˜µK
        5. é‡æ„çº¦æŸï¼šK * psi_i â‰ˆ psi_{i+1}
        
        Args:
            state_dim (int): åŸå§‹çŠ¶æ€Xç»´åº¦ï¼ˆè½¨é“6ç»´ï¼‰
            obs_dim (int): Koopmanè§‚æµ‹ç©ºé—´Psiç»´åº¦
            k_latent_dim (int): Koopmanå¾®åˆ†ç®—å­æ½œåœ¨ç»´åº¦
            hidden_dim (int): éšè—å±‚ç¥ç»å…ƒæ•°é‡
            max_file_id (int): æ–‡ä»¶IDçš„æœ€å¤§å–å€¼ï¼ˆç”¨äºEmbeddingå±‚ï¼‰
        """
        super(KoopmanCVAE, self).__init__()
        
        self.state_dim = state_dim
        self.obs_dim = obs_dim
        self.k_latent_dim = k_latent_dim
        self.max_file_id = max_file_id
        
        # ========== æ ¸å¿ƒæ–°å¢ï¼šFile ID Embeddingå±‚ ==========
        # å°†ç¦»æ•£çš„file_idè½¬æ¢ä¸ºè¿ç»­å‘é‡ï¼ˆé€‚é…CVAEè§£ç å™¨è¾“å…¥ï¼‰
        self.file_id_embedding = nn.Embedding(
            num_embeddings=max_file_id + 1,  # IDèŒƒå›´ï¼š0 ~ max_file_id
            embedding_dim=32  # åµŒå…¥ç»´åº¦ï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
        )
        self.embedding_dim = 32  # è®°å½•åµŒå…¥ç»´åº¦
        
        # 1. ç¼–ç å™¨ F: X â†’ Psiï¼ˆå®Œå…¨ä¿ç•™åŸæœ‰ç»“æ„ï¼‰
        self.encoder_f = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(), 
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, obs_dim)
        )
        
        # 2. CVAEç®—å­æ¨ç†ç½‘ç»œï¼ˆç¼–ç å™¨ï¼‰ï¼š(psi_i, psi_{i+1}, Î”t) â†’ (mu, log_var)
        # è¾“å…¥ï¼špsi_i + psi_{i+1} + Î”tï¼ˆè½¬ç§»å¯¹ä¸‰è¦ç´ ï¼‰
        self.cvae_encoder = nn.Sequential(
            nn.Linear(obs_dim * 2 + 1, hidden_dim * 2),  # obs_dim*2=psi_i+psi_{i+1}, +1=Î”t
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim * 2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, k_latent_dim * 2)  # è¾“å‡ºå¾®åˆ†ç®—å­åˆ†å¸ƒå‚æ•°
        )
        
        # 3. CVAEç®—å­ç”Ÿæˆç½‘ç»œï¼ˆè§£ç å™¨ï¼‰ï¼šz + æ ‡ç­¾(file_id_emb, Î”t) â†’ KoopmançŸ©é˜µ
        # æ ‡ç­¾ç»´åº¦ï¼šembedding_dim(file_id) + 1(Î”t) ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼‰
        self.cvae_decoder = nn.Sequential(
            nn.Linear(k_latent_dim + self.embedding_dim + 1, hidden_dim * 2),  # z + æ ‡ç­¾
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim * 2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, obs_dim * obs_dim)  # è¾“å‡ºKoopmançŸ©é˜µå±•å¹³
        )
        
        # 4. è§£ç å™¨ F': Psi â†’ Xï¼ˆå®Œå…¨ä¿ç•™åŸæœ‰ç»“æ„ï¼‰
        self.decoder_f_prime = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, state_dim)
        )

    def reparameterize(self, mu, log_var):
        """é‡å‚æ•°åŒ–æŠ€å·§ï¼ˆæ— ä¿®æ”¹ï¼‰"""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def get_koopman_matrix(self, z, file_id, delta_t):
        """
        CVAEç”ŸæˆKoopmanç®—å­çŸ©é˜µï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šæ ‡ç­¾æ”¹ä¸ºfile_id + Î”tï¼‰
        Args:
            z: (batch_size, k_latent_dim) é‡é‡‡æ ·çš„æ½œåœ¨ç‰¹å¾
            file_id: (batch_size,) LongTensor - æ–‡ä»¶IDï¼ˆç¦»æ•£æ•´æ•°ï¼‰
            delta_t: (batch_size, 1) FloatTensor - æ—¶é—´é—´éš”
        Returns:
            K_matrix: (batch_size, obs_dim, obs_dim) Koopmanç®—å­çŸ©é˜µ
        """
        batch_size = z.size(0)
        
        # ========== æ ¸å¿ƒä¿®æ”¹ï¼šå¤„ç†file_idå¹¶æ„å»ºæ–°æ ‡ç­¾ ==========
        # 1. File IDåµŒå…¥ï¼ˆç¦»æ•£â†’è¿ç»­ï¼‰
        file_id_emb = self.file_id_embedding(file_id)  # (batch_size, embedding_dim)
        
        # 2. æ„å»ºCVAEæ ‡ç­¾ï¼šfile_id_emb + Î”t
        label = torch.cat([file_id_emb, delta_t], dim=1)  # (batch_size, embedding_dim + 1)
        
        # 3. æ‹¼æ¥æ½œåœ¨ç‰¹å¾å’Œæ ‡ç­¾
        z_with_label = torch.cat([z, label], dim=1)  # (batch_size, k_latent_dim + embedding_dim + 1)
        
        # 4. ç”ŸæˆKoopmançŸ©é˜µ
        k_flat = self.cvae_decoder(z_with_label)
        K_matrix = k_flat.view(batch_size, self.obs_dim, self.obs_dim)
        
        return K_matrix
    
    # ===================== æ–°å¢æ–¹æ³•ï¼šè®¡ç®—KçŸ©é˜µé€æ¬¡ä¹˜ç§¯ï¼ˆé€‚é…éç›¸é‚»çŠ¶æ€å¯¹ï¼‰ =====================
    def compute_k_product(self, k_matrices_list):
        """
        æ‰¹é‡è®¡ç®—å¤šæ­¥KçŸ©é˜µçš„é€æ¬¡ä¹˜ç§¯ï¼ˆç”¨äºéç›¸é‚»çŠ¶æ€å¯¹çš„å¤šæ­¥è¯¯å·®è®¡ç®—ï¼‰
        Args:
            k_matrices_list: list of tensorï¼Œæ¯ä¸ªtensorå½¢çŠ¶ä¸º(batch_size, obs_dim, obs_dim)
                            å¯¹åº”(Xi,Xi+1),(Xi+1,Xi+2)...(Xj-1,Xj)çš„KçŸ©é˜µ
        Returns:
            K_product: (batch_size, obs_dim, obs_dim) é€æ¬¡ä¹˜ç§¯ç»“æœ K_i * K_{i+1} * ... * K_{j-1}
        """
        if not k_matrices_list:  # ç©ºåˆ—è¡¨å…œåº•
            return torch.eye(self.obs_dim).unsqueeze(0).repeat(
                1, 1, 1).to(next(self.parameters()).device)
        
        # åˆå§‹åŒ–ä¹˜ç§¯ä¸ºå•ä½çŸ©é˜µï¼ˆåŒ¹é…batch_sizeï¼‰
        batch_size = k_matrices_list[0].shape[0]
        K_product = torch.eye(self.obs_dim).unsqueeze(0).repeat(
            batch_size, 1, 1).to(k_matrices_list[0].device)
        
        # é€æ¬¡çŸ©é˜µä¹˜æ³•ï¼ˆæŒ‰é¡ºåºç›¸ä¹˜ï¼‰
        for K in k_matrices_list:
            K_product = torch.bmm(K_product, K)
        
        return K_product

    def forward_single_pair(self, x_i, x_i_plus_1, delta_t, file_id):
        """
        å•è½¬ç§»å¯¹å‰å‘ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šæ–°å¢file_idè¾“å…¥ï¼‰
        Args:
            x_i: (batch_size, state_dim) å‰ä¸€æ—¶åˆ»çŠ¶æ€
            x_i_plus_1: (batch_size, state_dim) åä¸€æ—¶åˆ»çŠ¶æ€
            delta_t: (batch_size, 1) æ—¶é—´é—´éš”
            file_id: (batch_size,) LongTensor - æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ–‡ä»¶ID
        Returns:
            psi_i: X_içš„ç¼–ç 
            psi_i_plus_1: X_{i+1}çš„ç¼–ç 
            K_matrix: å¯¹åº”æ ‡ç­¾çš„Koopmanç®—å­
            psi_recon: K*psi_iï¼ˆé‡æ„åpsi_{i+1}ï¼‰
            x_i_recon: psi_iè§£ç å›X_i
            x_i_plus_1_recon: psi_reconè§£ç å›X_{i+1}
            mu/log_var: å¾®åˆ†ç®—å­åˆ†å¸ƒå‚æ•°
        """
        # ========== å¼ºåˆ¶ç»Ÿä¸€æ‰€æœ‰è¾“å…¥ç»´åº¦ ==========
        # ç¡®ä¿çŠ¶æ€é‡æ˜¯2D (B, 6)
        if x_i.dim() == 1:
            x_i = x_i.unsqueeze(0)  # å•æ ·æœ¬ï¼š(6,) â†’ (1,6)
        if x_i_plus_1.dim() == 1:
            x_i_plus_1 = x_i_plus_1.unsqueeze(0)  # (6,) â†’ (1,6)
        
        # ç¡®ä¿delta_tæ˜¯2D (B, 1)
        if delta_t.dim() == 1:
            delta_t = delta_t.unsqueeze(1)  # (B,) â†’ (B,1) æˆ– (1,) â†’ (1,1)
        
        # ç¡®ä¿file_idæ˜¯1D LongTensor (B,)
        if file_id.dim() > 1:
            file_id = file_id.squeeze(-1)  # å»é™¤å¤šä½™ç»´åº¦
        if not file_id.dtype == torch.long:
            file_id = file_id.long()
        
        # ========== åŸæœ‰é€»è¾‘ï¼šç¼–ç çŠ¶æ€é‡ ==========
        psi_i = self.encoder_f(x_i)          # 2D (B, obs_dim)
        psi_i_plus_1 = self.encoder_f(x_i_plus_1)  # 2D (B, obs_dim)
        
        # ========== CVAEç¼–ç å™¨ï¼šæ¨ç†å¾®åˆ†ç®—å­åˆ†å¸ƒ ==========
        cvae_input = torch.cat([psi_i, psi_i_plus_1, delta_t], dim=1)
        k_params = self.cvae_encoder(cvae_input)
        mu, log_var = torch.chunk(k_params, 2, dim=1)
        
        # ========== é‡é‡‡æ ·æ½œåœ¨ç‰¹å¾z ==========
        z = self.reparameterize(mu, log_var)
        
        # ========== æ ¸å¿ƒä¿®æ”¹ï¼šç”ŸæˆKoopmanç®—å­ï¼ˆä½¿ç”¨file_id + Î”tä½œä¸ºæ ‡ç­¾ï¼‰ ==========
        K_matrix = self.get_koopman_matrix(z, file_id, delta_t)
        
        # ========== å•æ­¥é‡æ„ä¸è§£ç  ==========
        # 1. å•æ­¥é‡æ„ï¼šK*psi_i â‰ˆ psi_{i+1}
        psi_recon = torch.bmm(K_matrix, psi_i.unsqueeze(2)).squeeze(2)
        
        # 2. è§£ç å›Xç©ºé—´
        x_i_recon = self.decoder_f_prime(psi_i)
        x_i_plus_1_recon = self.decoder_f_prime(psi_recon)
        
        return psi_i, psi_i_plus_1, K_matrix, psi_recon, x_i_recon, x_i_plus_1_recon, mu, log_var

    def predict_long_term(self, x0, delta_t_seq, file_id, steps, use_prior=True, z_mean=None):
        """
        ä¿®æ­£åé•¿æ—¶é¢„æµ‹ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šæ–°å¢file_idè¾“å…¥ï¼‰
        Args:
            x0: (1, state_dim) åˆå§‹çŠ¶æ€ï¼ˆbatch_size=1ï¼‰
            delta_t_seq: (N, 1) æ—¶é—´é—´éš”åºåˆ—ï¼ˆnumpy/tensorï¼‰
            file_id: int/LongTensor - é¢„æµ‹ä½¿ç”¨çš„æ–‡ä»¶IDï¼ˆå•ä¸ªå€¼ï¼‰
            steps: int é¢„æµ‹æ­¥æ•°
            use_prior: bool æ˜¯å¦ä½¿ç”¨å…ˆéªŒåˆ†å¸ƒï¼ˆN(0,1)ï¼‰é‡‡æ ·zï¼ŒFalseåˆ™ç”¨è¾“å…¥çš„z_mean
            z_mean: (1, k_latent_dim) è‡ªå®šä¹‰æ½œåœ¨ç‰¹å¾ï¼ˆå¦‚è®­ç»ƒæ—¶çš„å‡å€¼ï¼‰
        Returns:
            trajectory: (steps+1, state_dim) é¢„æµ‹è½¨è¿¹
            K_matrices: list æ¯æ­¥çš„Koopmanç®—å­
        """
        self.eval()
        # æ ¸å¿ƒï¼šæå‰è·å–åŸºå‡†è®¾å¤‡ï¼ˆä»x0æå–ï¼Œç¡®ä¿æ‰€æœ‰å¼ é‡ç»Ÿä¸€ï¼‰
        device = x0.device
        trajectory = [x0[0].cpu().numpy()]  # åˆå§‹çŠ¶æ€
        K_matrices = []

        print("\n" + "="*50)
        print("ğŸ” delta_t_seq åºåˆ—è¯¦ç»†ä¿¡æ¯")
        print("="*50)
        
        # 1. å¤„ç†Î”tåºåˆ—
        if isinstance(delta_t_seq, np.ndarray):
            print(f"åŸå§‹ç±»å‹ï¼šnumpy.ndarray | åŸå§‹å½¢çŠ¶ï¼š{delta_t_seq.shape}")
            delta_t_seq_tensor = torch.FloatTensor(delta_t_seq).to(device)
        elif isinstance(delta_t_seq, torch.Tensor):
            print(f"åŸå§‹ç±»å‹ï¼štorch.Tensor | åŸå§‹è®¾å¤‡ï¼š{delta_t_seq.device} | åŸå§‹å½¢çŠ¶ï¼š{delta_t_seq.shape}")
            delta_t_seq_tensor = delta_t_seq.to(device)
        else:
            print(f"åŸå§‹ç±»å‹ï¼š{type(delta_t_seq)}ï¼ˆä¸æ”¯æŒï¼‰")
            delta_t_seq_tensor = torch.tensor([], device=device)
        
        # 2. å¤„ç†file_idï¼ˆç¡®ä¿æ˜¯1D LongTensorï¼‰
        if isinstance(file_id, int):
            file_id_tensor = torch.tensor([file_id], dtype=torch.long, device=device)
        else:
            file_id_tensor = file_id.to(device).long()
            if file_id_tensor.dim() == 0:
                file_id_tensor = file_id_tensor.unsqueeze(0)
        
        # 3. ç»Ÿä¸€æ ¼å¼åçš„ä¿¡æ¯
        if len(delta_t_seq_tensor) > 0:
            delta_t_seq_flat = delta_t_seq_tensor.cpu().numpy().flatten()
            print(f"ç»Ÿä¸€åå½¢çŠ¶ï¼š{delta_t_seq_tensor.shape} | å±•å¹³åé•¿åº¦ï¼š{len(delta_t_seq_flat)}")
            print(f"é¢„æµ‹æ­¥æ•°ï¼š{steps} | Î”tåºåˆ—é•¿åº¦æ˜¯å¦åŒ¹é…ï¼š{'âœ…' if len(delta_t_seq_flat) >= steps else 'âŒ'}")
            
            # æ‰“å°å‰100ä¸ªå€¼
            print("\nğŸ“Š å‰100ä¸ªÎ”tå€¼ï¼ˆæ¯è¡Œ10ä¸ªï¼‰ï¼š")
            print_limit = min(100, len(delta_t_seq_flat))
            delta_t_print = delta_t_seq_flat[:print_limit]
            for i in range(0, print_limit, 10):
                end_idx = min(i+10, print_limit)
                dt_line = [f"{dt:.6f}" for dt in delta_t_print[i:end_idx]]
                print(f"  ç¬¬{i+1}-{end_idx}ä¸ªï¼š{', '.join(dt_line)}")
        else:
            print("âŒ Î”tåºåˆ—ä¸ºç©ºï¼")
        print("="*50 + "\n")

        x_curr = x0.to(device)  # åˆå§‹çŠ¶æ€
        
        with torch.no_grad():
            for i in range(steps):
                # 1. è·å–å½“å‰æ­¥çš„Î”t
                take_len = min(i, len(delta_t_seq))
                delta_t_prev = delta_t_seq_tensor[:take_len] if take_len > 0 else torch.tensor([[0.0]], device=device)
                delta_t_sum = delta_t_prev.sum().item()
                delta_t = torch.tensor([[delta_t_sum]], dtype=torch.float32, device=device)
                
                # 2. æ½œåœ¨ç©ºé—´é‡‡æ ·
                if use_prior:
                    z = torch.randn(1, self.k_latent_dim, dtype=torch.float32, device=device)
                else:
                    if z_mean is None:
                        z = torch.zeros(1, self.k_latent_dim, dtype=torch.float32, device=device)
                    else:
                        z = z_mean.to(device)
                
                # 3. æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨file_id + Î”tç”ŸæˆKoopmanç®—å­
                K_matrix = self.get_koopman_matrix(z, file_id_tensor, delta_t)  # (1, obs_dim, obs_dim)
                K_matrices.append(K_matrix.cpu().numpy())
                
                # 4. Koopmanç®—å­æ˜ å°„ï¼špsi_curr â†’ psi_next
                psi_curr = self.encoder_f(x_curr)  # (1, obs_dim)
                psi_next = torch.bmm(K_matrix, psi_curr.unsqueeze(2)).squeeze(2)  # (1, obs_dim)
                
                # 5. è§£ç å›Xç©ºé—´
                x_next = self.decoder_f_prime(psi_next)  # (1, state_dim)
                
                # 6. æ›´æ–°è½¨è¿¹å’Œå½“å‰çŠ¶æ€
                trajectory.append(x_next[0].cpu().numpy())
                #x_curr = x_next
        
        return np.array(trajectory), K_matrices
    

#  ==========================================
# æŸå¤±å‡½æ•°ï¼ˆé€‚é…æ ‡ç­¾ä¿®æ”¹ï¼Œä¿ç•™æŒ‰æ–‡ä»¶åˆ†ç»„çš„çº¦æŸï¼‰
# ==========================================
def loss_cvae_koopman(psi_recon, psi_true, 
                      x_i_recon, x_i_true,
                      x_i_plus_1_recon, x_i_plus_1_true,
                      mu, log_var, K_matrix,
                      file_ids,  # æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ–‡ä»¶ç¼–å· (batch_size,)
                      K_product=None, lambda_k_product=10.0,
                      alpha=1.0, beta=0.1, gamma=1, theta=0.1, delta=1.0):
    """
    æŸå¤±å‡½æ•°ï¼ˆé€‚é…æ ‡ç­¾ä¿®æ”¹ï¼Œä¿ç•™åŸæœ‰åˆ†ç»„çº¦æŸé€»è¾‘ï¼‰
    1. psi_recon_loss: K*psi_i â‰ˆ psi_{i+1}ï¼ˆæ ¸å¿ƒå•æ­¥é‡æ„ï¼‰
    2. x_recon_loss: Xé‡æ„æŸå¤±
    3. kl_loss: CVAEæ½œåœ¨åˆ†å¸ƒKLæ•£åº¦
    4. k_product_loss: å¤šæ­¥Kä¹˜ç§¯è¯¯å·®ï¼ˆä»…K_productä¸ä¸ºNoneæ—¶ç”Ÿæ•ˆï¼‰
    5. latent_consistency_loss: æŒ‰æ–‡ä»¶åˆ†ç»„çº¦æŸmu/log_varï¼ˆåŒæ–‡ä»¶å†…æ ·æœ¬æ¥è¿‘ï¼‰
    
    Args:
        file_ids: (batch_size,) tensor - æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ–‡ä»¶ç¼–å·ï¼ˆå¦‚0,0,1,1,2...ï¼‰
        K_product: (batch_size, obs_dim, obs_dim) å¤šæ­¥KçŸ©é˜µä¹˜ç§¯
        lambda_k_product: å¤šæ­¥Kä¹˜ç§¯è¯¯å·®çš„æƒé‡
        delta: æŒ‰æ–‡ä»¶åˆ†ç»„çš„ä¸€è‡´æ€§æŸå¤±æƒé‡
    """
    # 1. æ ¸å¿ƒé‡æ„æŸå¤±ï¼šK*psi_i â‰ˆ psi_{i+1}
    psi_recon_loss = F.mse_loss(psi_recon, psi_true)
    
    # 2. Xç©ºé—´é‡æ„æŸå¤±
    x_i_recon_loss = F.mse_loss(x_i_recon, x_i_true)
    x_i_plus_1_recon_loss = F.mse_loss(x_i_plus_1_recon, x_i_plus_1_true)
    x_recon_loss = (x_i_recon_loss + x_i_plus_1_recon_loss) / 2
    
    # 3. CVAE KLæ•£åº¦æŸå¤±
    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    kl_loss = kl_loss / psi_recon.size(0)
    
    # 4. å¤šæ­¥Kä¹˜ç§¯è¯¯å·®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    k_product_loss = torch.tensor(0.0).to(psi_recon.device)
    if K_product is not None:
        k_product_loss = F.mse_loss(K_matrix, K_product) * lambda_k_product
    
    # 5. æŒ‰æ–‡ä»¶åˆ†ç»„çš„mu/log_varä¸€è‡´æ€§æŸå¤±ï¼ˆåŸæœ‰é€»è¾‘ï¼Œä¿ç•™ï¼‰
    latent_consistency_loss = torch.tensor(0.0).to(psi_recon.device)
    unique_file_ids = torch.unique(file_ids)
    
    for fid in unique_file_ids:
        mask = (file_ids == fid)
        if not torch.any(mask):
            continue
        
        mu_group = mu[mask]
        log_var_group = log_var[mask]
        
        mu_group_mean = mu_group.mean(dim=0, keepdim=True)
        log_var_group_mean = log_var_group.mean(dim=0, keepdim=True)
        
        mu_consistency = F.mse_loss(mu_group, mu_group_mean.expand_as(mu_group))
        log_var_consistency = F.mse_loss(log_var_group, log_var_group_mean.expand_as(log_var_group))
        
        n_group = torch.sum(mask).float()
        latent_consistency_loss += (mu_consistency + log_var_consistency) * n_group
    
    latent_consistency_loss = (latent_consistency_loss / mu.size(0)) * delta

    # æ€»æŸå¤±
    total_loss = (psi_recon_loss * alpha + 
                  x_recon_loss * beta + 
                  kl_loss * gamma +
                  k_product_loss +
                  latent_consistency_loss)
    
    # è¿”å›æ‰€æœ‰æŸå¤±é¡¹
    return total_loss, psi_recon_loss, x_recon_loss, kl_loss, k_product_loss, latent_consistency_loss

